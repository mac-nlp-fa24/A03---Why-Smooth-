{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c924f0a-d2f7-4217-93e7-48c50d00a1a6",
   "metadata": {},
   "source": [
    "# Why Smooth N-Gram Models?\n",
    "\n",
    "The purpose of this assignment is to demonstrate how, but mostly *why* we choose to smooth an n-gram language model. \n",
    "\n",
    "In practice, this is driven primarily by the fact that our training corpora are limited in size. The number of sentences that are grammatical (and thus probably deserving of some probability!) are provably infinite and the number of sequences seen by our model are finite, so even in theory it's impossible for our n-gram models not to assign 0 probability to perfectly reasonable sentences. **This is a problem!**\n",
    "\n",
    "Our learning goals are to be able to:\n",
    "\n",
    "1. Describe the limitations of MLE estimation of parameters from corpora.\n",
    "2. Justify the idea of smoothing.\n",
    "3. Be able to motivate the design of particular smoothing techniques by making reference to the properties of n-gram models and language data.\n",
    "\n",
    "And additionally!\n",
    "\n",
    "4. Practice inspecting computational models and language data to answer questions about the behavior and limitations of those models.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06646b5-315a-4709-a6be-9f9f95aa76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"austen-emma.txt\") as corpus_f:\n",
    "    emma = corpus_f.read().split()\n",
    "\n",
    "print(\"There are {} unique words in Emma. This is our vocab size!\".format(\n",
    "    len(list(set(emma)))\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae549a7f-0500-4ac1-8b6b-4edde07c70da",
   "metadata": {},
   "source": [
    "In the following cell, copy over your code for counting the number of n-grams in a particular text from the prior activity. We're going to start looking at the number of *occurances* of particular ngrams in a text. In particular, get counts for **all of the unigrams, bigrams, and trigrams that appear in Emma**. Additionally, **compute the total number of unigrams/bigrams/trigrams observed in the data** (this should be fairly straightforward to compute, in code or by hand). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728cd882-24aa-4a2c-a5f3-2a0bde121405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping\n",
    "\n",
    "unigrams : Mapping[Sequence[str], int] = get_counts(emma, 1)\n",
    "bigrams : Mapping[Sequence[str], int] = get_counts(emma, 2)\n",
    "trigrams : Mapping[Sequence[str], int] = get_counts(emma, 3)\n",
    "\n",
    "#TODO: Complete the above task using prior code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8416c8ce-3b22-474a-8c8c-02062c79b23d",
   "metadata": {},
   "source": [
    "Now let's take a glance at what our non-zero counts look like! First plotting a histogram of the counts that correspond to each trigram, sorted by most to least frequent. **Please play around with the code to try and better understand what's going on!**\n",
    "\n",
    "\n",
    "**Note:** These plots will be very weird to read --- there are elements that are strange, and can be masked by the way we represent the data! That is intentional! Play with the code to get a feel for what the distribution looks like!\n",
    "\n",
    "For example, the large number of counts for certain n-grams makes others hard to see. Try plotting the *log* of these the counts instead! Check out bigram and trigram counts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ef1d8-4236-462e-aa93-f232ef69f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_counts(counts : Mapping[Sequence[str], int], cutoff = None):  \n",
    "    values = sorted(list(counts.values()), key = lambda x: -x)[:cutoff]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.stairs(values, fill=True)\n",
    "    plt.show()\n",
    "\n",
    "plot_counts(unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1859f54-06d1-48ec-a684-7b3ac4e110a8",
   "metadata": {},
   "source": [
    "Now lets look at how many n-grams have a *particular* count! \n",
    "\n",
    "I have commented out code that prints the counts in order from largest to smallest. This should answer a reasonable question about why the x-axis is so wide: There is, in fact, a single token with a very large count! **Play around and inspect the data more with your partner(s) to build an understanding of the distributional properties of the data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ee782-1fcb-48ba-a11f-5e3ae757fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_count_dist(counts : Mapping[Sequence[str], int]):  \n",
    "    values = list(counts.values())\n",
    "    #print(sorted(values, key = lambda x : -x))\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(values)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_count_dist(unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a3d14-372e-44ba-b09b-39de5ded3a91",
   "metadata": {},
   "source": [
    "Now, more of a combinatorics puzzle: **How many *possible* unigrams/bigrams/trigrams are there, given our vocabulary for Emma?**\n",
    "\n",
    "**Bonus**: Write code to enumerate all of them and modify the plotting code above to incorporate 0 counts! I give you a generic function starter code below to do this for any $n$, but feel free to substitute an implementation that directly generates the lists for $n = 1,2,3$ (though writing this function may be a fun exercise for dealing with this kind of combinatoric search!).\n",
    "\n",
    "Test on a smaller vocabulary than that of Emma, and ensure you get the right count!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f8837-faf4-4b1e-927e-f970fbff0e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Sequence\n",
    "\n",
    "def get_all_ngrams(vocab : Sequence[str], n : int) -> Iterable[Sequence[str]]:\n",
    "    # TODO\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d9cfb0-da91-403c-9996-911d18fa93c0",
   "metadata": {},
   "source": [
    "One thing to consider is how *sparse* our data is with respect to the number of parameters we need to estimate. Our n-gram model is a conditional probability distribution that must assign a probability to any assignment of words from our vocab to any one of the $n$ random variables. As the combinatorics puzzle should indicate, that quickly becomes a *lot* of parameters and for a fixed dataset, the number of n-gram observations doesn't really change! That means that an n-gram with a very small, but non-zero probability has a chance of showing up 0 times! \n",
    "\n",
    "To see the scale of these probabilities, we can think about the (joint) probability we assign to an n-gram that appears a single time in our corpus. This is the smallest probability an MLE n-gram model can assign to a n-gram! Compute that for unigram/bigram/trigrams using the counts you got from Emma!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b7c0de-b450-493b-9729-28d7a3d6c07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68642ec7-3fb1-4e41-9a80-22bf7680a7f1",
   "metadata": {},
   "source": [
    "Now let's consider a hypothetical model of language where every unigram/bigram/trigram was equally probable (i.e., *uniform*). This is obviously wrong (take a look at some of your possible bigrams/trigrams to convince yourself some should have 0 or near 0 probability!), but let's consider this just for the sake of building our intuitions. What probability should we assign to each unigram/bigram/trigram? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5688c62-6cfe-4369-ba2d-08b3d04f6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeac966-3a48-423d-8e22-7e43bf7cf43a",
   "metadata": {},
   "source": [
    "If your math is checking out, there should be orders of magnitude differences in these probabilities for larger n's! That means that even if a uniform distribution is silly, it's going to be hard to believe that our n-gram model has enough fidelity to represent the difference between an n-gram that should never exist (has probability 0) and one that is very unlikely (i.e., should have a small, but >0 probability). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc8a25f-f6bf-47ff-8411-2dca8813a34b",
   "metadata": {},
   "source": [
    "The exercises above are trying to convince you of a few things.\n",
    "\n",
    "First, that models trained on finite data will underestimate the probability of rare n-grams that are not observed. This motivates smoothing in general!\n",
    "\n",
    "Second, that part of this limitation comes from the fact that the smallest probability that we can assign in an MLE model is constrained. **How is it constrained?** \n",
    "\n",
    "This observation can be used to motivate (a) smoothing method(s) discussed in the reading (and in the homework!). **Which one(s)? How?**\n",
    "\n",
    "Third, that there is a relationship between the hyperparameter $n$ for an n-gram model and the quality of the estimation of each parameter/probability we estimate for a given corpus. **What is that relationship?** \n",
    "\n",
    "This observation motivates (a) smoothing method(s) discussed in the reading and homework. **Which one(s)? What is the motivation?**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
